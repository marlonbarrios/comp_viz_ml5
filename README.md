# comp_viz_ml5

# XR/AR Machine Learning 
![Screen Shot 2022-06-28 at 5 45 40 PM](https://user-images.githubusercontent.com/90220317/176305847-ed74aaff-c260-45f8-803f-6b4f438a02db.png)

[Back to Portfolio](https://marlonbarrios.github.io/)

[What is ml5.js](https://ml5js.org/) 

## Interactive 3D animation AR with hand pose recognition using ML:
App that places and animated 3D model on an image target and recognizes hand poses using tensorflow.js; when a handpose for the user  is recognized it triggers different behaviors in the model: wave, jump, thumbs up and die!!


<img width="1152" alt="Screen Shot 2022-11-24 at 8 54 57 AM" src="https://user-images.githubusercontent.com/90220317/203802593-cb2e7e02-bf17-4351-ab5f-8a25626dd7d4.png">

Load marker image in your phone  and load  the app in the browser.
![316008299_10162261094790968_795135122697280972_n](https://user-images.githubusercontent.com/90220317/203802706-cec16101-a342-4de4-847c-41f7c1905b29.jpg)


[Live app here](https://marlonbarrios.github.io/tensorflow_handgestures/)

[Repository here](https://github.com/marlonbarrios/tensorflow_handgestures/) 

## AR+ MediaPipe Face Mesh + AI generated mask:
I tested today  a masks filter generated by an AI as a texture for a facemesh detection anchor and estimating 468 3D face landmarks in real-time even on mobile devices. It employs machine learning (ML) to infer the 3D facial surface, requiring only a single camera input without the need for a dedicated depth sensor. I used a  png as a mask with the template provided by Spark AR. I used the MindAR library and Three.js
See AI generated image here with the natural language prompt
wolfram rule 30 colorful mask

https://creator.nightcafe.studio/cre.../AA9oLVcj0WqJm5p43ISn

![Screen Shot 2022-11-15 at 9 22 33 AM](https://user-images.githubusercontent.com/90220317/203803893-3122e48f-5a28-4f69-94d3-0ab2a819ea38.png)


[Live app here](https://marlonbarrios.github.io/ace_mesh_masks_ar)

[Repository here](https://github.com/marlonbarrios/face_mesh_masks_ar) 


## Video Hot Spots |

Recreating interactive triggering of sounds I used to do in MaxMspJitter (2003) using now  JavaScript. The motion is detected by progressive frame difference triggering several oscillators square wave forms.
I am using  VIDA is a  library that adds camera (or video) based motion detection and blob tracking functionality to p5js. 
Mind blowing that it runs in the browser.


<img width="630" alt="Screen Shot 2022-11-04 at 3 52 11 PM" src="https://user-images.githubusercontent.com/90220317/200063274-ebe9e3a1-9c6d-49e3-a9ef-62f9e4ad8e93.png">

[Live App Here](https://marlonbarrios.github.io/cv_hotspots_camera_vida/) 

[Repository here](https://github.com/marlonbarrios/cv_hotspots_camera_vida) 


## Experiment with ML5: Hand Detection

Can I map the detected landmarks of a hand from the video to draw simple geometries on a canvas and suggest the presence of a hand? 
Part 2 of the computer vision adventures in p5 and Media Pipe/ML5. It blows my mind that this is running in the browser!
Have fun!!!

![Screen Shot 2022-06-28 at 5 46 18 PM](https://user-images.githubusercontent.com/90220317/176305909-cb7c2288-fe06-4135-aaf3-ac942542bb0f.png)

PS: I takes 20 seconds to load the api.

#cv #handdetection #p5 #javascript

[Live App Here](https://marlonbarrios.github.io/cv-p5-hand-tracking/) 

[Repository here](https://github.com/marlonbarrios/cv-p5-hand-tracking) 


## Experiment with ML5: Face Detection 

A a text is writen when a smile is detected.

![Screen Shot 2022-06-28 at 5 57 13 PM](https://user-images.githubusercontent.com/90220317/176307535-1e15a535-4fc4-47a5-96d4-f329a43f62c5.png)

[Live App Here](https://marlonbarrios.github.io/cv-face-detection-p5/) 

[Repository here](https://github.com/marlonbarrios/cv-face-detection-p5) 

## Nose Tracking with Simple Noise |
This app tracks the nose doing face detection with machine learning. The position of the 'nose' in the frame trigger oscillators sounds when inside the hot spots areas. Very simple interactivity but  coded from scratch detecting complex human features with Java Script and all running in the browser.
And of course, a red circle is mapped  where your nose is located.
Code by Marlon Barrios Solano as part of the SoftSpaces series.

Live app here:

[Live App Here](https://marlonbarrios.github.io/nosetrackingsimplesound/) 

[Repository here](https://github.com/marlonbarrios/nosetrackingsimplesound) 


## I am... |
Face tracking using ML5 and mapping a mask responsive to size changes. All in Java Script.

[Live App Here](https://marlonbarrios.github.io/face_detection_anonymousmask/) 

[Repository here](https://github.com/marlonbarrios/face_detection_anonymousmask) 


## Day of the Dead!

[Live App Here](https://marlonbarrios.github.io/diadelosmuertos/) 

[Back to Portfolio](https://github.com/marlonbarrios/diadelosmuertos)
